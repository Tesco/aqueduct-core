package com.tesco.aqueduct.pipe.storage

import com.opentable.db.postgres.junit.EmbeddedPostgresRules
import com.opentable.db.postgres.junit.SingleInstancePostgresRule
import com.tesco.aqueduct.pipe.api.Message
import com.tesco.aqueduct.pipe.api.MessageResults
import groovy.sql.Sql
import org.junit.ClassRule
import spock.lang.AutoCleanup
import spock.lang.Shared

import javax.sql.DataSource
import java.sql.Connection
import java.sql.PreparedStatement
import java.sql.ResultSet
import java.sql.Timestamp
import java.time.ZonedDateTime

class PostgresqlStorageIntegrationSpec extends StorageSpec {

    // Starts real PostgreSQL database, takes some time to create it and clean it up.
    @Shared @ClassRule
    SingleInstancePostgresRule pg = EmbeddedPostgresRules.singleInstance()

    @AutoCleanup
    Sql sql

    PostgresqlStorage storage
    DataSource dataSource

    long retryAfter = 5000

    long batchSize = 1000
    long maxOverheadBatchSize = (Message.MAX_OVERHEAD_SIZE * limit) + batchSize

    def setup() {
        sql = new Sql(pg.embeddedPostgres.postgresDatabase.connection)

        dataSource = Mock()
        dataSource.connection >> pg.embeddedPostgres.postgresDatabase.connection

        sql.execute("""
        DROP TABLE IF EXISTS EVENTS;
          
            CREATE TABLE EVENTS(
                msg_offset bigint PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY NOT NULL,
                msg_key varchar NOT NULL, 
                content_type varchar NOT NULL, 
                type varchar NOT NULL, 
                created_utc timestamp NOT NULL,
                tags JSONB NULL, 
                data text NULL,
                event_size int NOT NULL
            );
        """)

        storage = new PostgresqlStorage(dataSource, limit, retryAfter, batchSize)
    }

    def "get the latest offset where tags contains type but no other keys"() {
        given: "there is postgres storage"
        def dataSourceWithMockedConnection = Mock(DataSource)
        def postgresStorage = new PostgresqlStorage(dataSourceWithMockedConnection, limit, 0, batchSize)

        and: "a mock connection is provided when requested"
        def connection = Mock(Connection)
        dataSourceWithMockedConnection.getConnection() >> connection

        and: "a connection returns a prepared statement"
        def preparedStatement = Mock(PreparedStatement)
        preparedStatement.executeQuery() >> Mock(ResultSet)
        connection.prepareStatement(_ as String) >> preparedStatement

        when: "requesting the latest matching offset with tags specifying a type key"
        postgresStorage.getLatestOffsetMatching(["some_type"])

        then: "a query is created that does not contain tags in the where clause"
        1 * preparedStatement.setString(1, "some_type")
        0 * preparedStatement.setString(_ as Integer, '{}')
    }

    def "get messages where tags contains type but no other keys"() {
        given: "there is postgres storage"
        def limit = 1
        def dataSourceWithMockedConnection = Mock(DataSource)
        def postgresStorage = new PostgresqlStorage(dataSourceWithMockedConnection, limit, 0, batchSize)

        and: "a mock connection is provided when requested"
        def connection = Mock(Connection)
        dataSourceWithMockedConnection.getConnection() >> connection

        and: "a connection returns a prepared statement"
        def preparedStatement = Mock(PreparedStatement)
        preparedStatement.executeQuery() >> Mock(ResultSet)
        connection.prepareStatement(_ as String) >> preparedStatement

        when: "requesting messages with tags specifying a type key"
        postgresStorage.read(["some_type"], 0)

        then: "a query is created that does not contain tags in the where clause"
        1 * preparedStatement.setString(1, "some_type")
        0 * preparedStatement.setString(_ as Integer, '{}')
    }

    def "the messages returned are no larger than the maximum batch size when reading without a type"() {
        given: "there are messages with unique keys"
        def msg1 = message(key: "x")
        def msg2 = message(key: "y")
        def msg3 = message(key: "z")

        and: "the size of each message is set so that 3 messages are just larger than the max overhead batch size"
        int messageSize = Double.valueOf(maxOverheadBatchSize / 3).intValue() + 1

        and: "they are inserted into the integrated database"
        insert(msg1, messageSize)
        insert(msg2, messageSize)
        insert(msg3, messageSize)

        when: "reading from the database"
        MessageResults result = storage.read([], 0)

        then: "messages that are returned are no larger than the maximum batch size when reading with a type"
        result.messages.size() == 2
    }

    def "the messages returned are no larger than the maximum batch size"() {
        given: "there are messages with unique keys"
        def msg1 = message(key: "x", type: "type-1")
        def msg2 = message(key: "y", type: "type-1")
        def msg3 = message(key: "z", type: "type-1")

        and: "the size of each message is set so that 3 messages are just larger than the max overhead batch size"
        int messageSize = Double.valueOf(maxOverheadBatchSize / 3).intValue() + 1

        and: "they are inserted into the integrated database"
        insert(msg1, messageSize)
        insert(msg2, messageSize)
        insert(msg3, messageSize)

        when: "reading from the database"
        MessageResults result = storage.read(["type-1"], 0)

        then: "messages that are returned are no larger than the maximum batch size"
        result.messages.size() == 2
    }

    def "retry-after is zero if the pipe is not empty"() {
        given: "I have some records in the integrated database"
        insert(message(key: "z"))
        insert(message(key: "y"))
        insert(message(key: "x"))

        when:
        MessageResults result = storage.read([], 0)

        then:
        result.retryAfterSeconds == 0
        result.messages.size() == 3
    }

    def "retry-after is non-zero if the pipe has no more data at specified offset"() {
        given: "I have some records in the integrated database"
        insert(message(key: "z"))
        insert(message(key: "y"))
        insert(message(key: "x"))

        when:
        MessageResults result = storage.read([], 4)

        then:
        result.retryAfterSeconds > 0
        result.messages.isEmpty()
    }

    def "retry-after is non-zero if the pipe has no data"() {
        given: "I have no records in the integrated database"

        when:
        MessageResults result = storage.read([], 0)

        then:
        result.retryAfterSeconds > 0
        result.messages.isEmpty()
    }


    //TODO: this test should work but doesn't
    def 'All duplicate messages are compacted for whole data store'() {
        given: 'an existing data store with duplicate messages for the same key'
        insert(message(1, "type", "A","content-type", ZonedDateTime.parse("2000-12-01T10:00:00Z"), "data"))
        insert(message(2, "type", "A","content-type", ZonedDateTime.parse("2000-12-01T10:00:00Z"), "data"))
        insert(message(3, "type", "B","content-type", ZonedDateTime.parse("2000-12-01T10:00:00Z"), "data"))

        when: 'compaction is run on the whole data store'
        storage.compactUpTo(ZonedDateTime.parse("2000-12-02T10:00:00Z"))

        and: 'all messages are requested'
        MessageResults result = storage.read(["type"], 0)
        List<Message> retrievedMessages = result.messages

        then: 'duplicate messages are deleted'
        retrievedMessages.size() == 2

        and: 'the correct compacted message list is returned in the message results'
        result.messages*.offset*.intValue() == [2, 3]
        result.messages*.key == ["A", "B"]

    }

    def 'All duplicate messages are compacted to a given offset with 3 duplicates'() {
        given: 'an existing data store with duplicate messages for the same key'

        when: 'compaction is run up to the timestamp of offset 1'

        and: 'all messages are requested'


        then: 'duplicate messages are not deleted as they are beyond the threshold'

    }

    def 'All duplicate messages are compacted to a given offset, complex case'() {
        given: 'an existing data store with duplicate messages for the same key'



        when: 'compaction is run up to the timestamp of offset 4'


        and: 'all messages are requested'


        then: 'duplicate messages are deleted that are within the threshold'

    }

    @Override
    void insert(Message msg, int maxMessageSize=0, def time = Timestamp.valueOf(msg.created.toLocalDateTime()) ) {

        if (msg.offset == null) {
            sql.execute(
                "INSERT INTO EVENTS(msg_key, content_type, type, created_utc, data, event_size) VALUES(?,?,?,?,?,?);",
                msg.key, msg.contentType, msg.type, time, msg.data, maxMessageSize
            )
        } else {
            sql.execute(
                "INSERT INTO EVENTS(msg_offset, msg_key, content_type, type, created_utc, data, event_size) VALUES(?,?,?,?,?,?,?);",
                msg.offset, msg.key, msg.contentType, msg.type, time, msg.data, maxMessageSize
            )
        }
    }
}
